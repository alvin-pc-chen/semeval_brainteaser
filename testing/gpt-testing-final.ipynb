{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from openai import OpenAI\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from utils import constants\n",
    "from utils import util\n",
    "from utils import keys\n",
    "from utils import prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'In a small village, two farmers are working in their fields - a diligent farmer and a lazy farmer. The hardworking farmer is the son of the lazy farmer, but the lazy farmer is not the father of the hardworking farmer. Can you explain this unusual relationship?', 'choice_list': ['The lazy farmer is his mother.', 'The lazy farmer is not a responsible father as he is lazy.', 'The diligent farmer devoted himself to the farm and gradually forgot his father.', 'None of above.']}\n"
     ]
    }
   ],
   "source": [
    "# Load data and Initialize OpenAI\n",
    "os.environ['OPENAI_API_KEY'] = keys.WX\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "with open(\"../data/new_test_data_nolabel/SP_new_test_corrected.json\") as f:\n",
    "    data = f.read()\n",
    "data = json.loads(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "SYSTEM_PROMPT = {\"role\": \"system\", \n",
    "                 \"content\": \"You are a Question Answering Model, your response must be a number from the choices that are delimited by the symbol \\\";\\\" .\"}\n",
    "MULTI_PREFIX = \"You are a Question Answering Model, your response must be a number from the choices that are delimited by the symbol \\\";\\\". Here are some examples: \\n\"\n",
    "SP_QUESTION = \"Think outside of the box and respond with the number corresponding to the best choice for the following question.\\nQuestion: \"\n",
    "WP_QUESTION = \"For the following word problem, look at the meaning and letters in the words and respond with the number corresponding to the best choice.\\nQuestion: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Multishot GPT3.5-TURBO\n",
    "preds = util.run_eval_cot(client, MODEL, data, prompts.CHAIN_SP_BASE, prompts.CHAIN_SYSTEM, MULTI_PREFIX, m=0, n=len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission to /Users/alvinchen/Documents/GitHub/brainteaser-data/submission/answer_sen.txt\n"
     ]
    }
   ],
   "source": [
    "util.submission_log(preds, \"answer_sen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(\"test_logs/gpt-3.5-turbo_eval_2023-12-18_21-48-05.log\", \"r\") as f:\n",
    "    raw = f.readlines()\n",
    "lines = raw[30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy answers and ids\n",
    "ans = {}\n",
    "for i in range(len(lines)-1):\n",
    "    m = re.search(r\"Question [0-9]+:\", lines[i])\n",
    "    if m is not None:\n",
    "        n = m.group(0)\n",
    "        id = int(re.search(r\"[0-9]+\", n).group(0))\n",
    "        an = re.search(r\"answer is [0-3]\", lines[i+1])\n",
    "        if an is not None:\n",
    "            an = int(an.group(0)[-1])\n",
    "            ans[id] = an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 23, 26, 28, 31, 34, 38, 58, 59, 92, 95, 114]\n"
     ]
    }
   ],
   "source": [
    "notin = []\n",
    "for i in range(1, 120):\n",
    "    if i not in ans:\n",
    "        notin.append(i)\n",
    "print(notin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 47, 98, 39, 5, 23, 3, 102, 51, 28, 50, 77, 1, 24, 88, 94, 71, 85, 11, 111, 93, 86, 119, 66, 104, 108, 61, 60, 116, 73, 59, 110, 52, 72, 36, 17, 92, 21, 69, 62, 113, 107, 90, 81, 112, 35, 16, 57, 19, 40, 43, 70, 46, 79, 13, 83, 64, 18, 49, 27, 58, 115, 31, 33, 2, 109, 6, 9, 8, 97, 84, 41, 89, 10, 101, 91, 53, 106, 55, 117, 82, 99, 37, 56, 87, 105, 44, 100, 67, 22, 20, 15, 12, 103, 75, 4, 7, 14, 29, 34, 32, 42, 65, 95, 25, 96, 114, 30, 54, 118, 63, 76, 0, 26, 74, 78, 45, 80, 68, 48]\n"
     ]
    }
   ],
   "source": [
    "repeatid = []\n",
    "test_questions = []\n",
    "order = []\n",
    "for data in sp_test:\n",
    "    test_questions.append(data[\"question\"])\n",
    "for data in sp_all:\n",
    "    if data[\"question\"] in test_questions:\n",
    "        repeatid.append(data[\"id\"])\n",
    "        m = [i for i in range(len(test_questions)) if test_questions[i] == data[\"question\"]][0]\n",
    "        order.append(m)\n",
    "print(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "label = []\n",
    "choices = []\n",
    "for i in order:\n",
    "    id = repeatid[i]\n",
    "    for data in sp_all:\n",
    "        if data[\"id\"] == id:\n",
    "            correct.append(data[\"answer\"])\n",
    "            label.append(data[\"label\"])\n",
    "            choices.append(\"\".join(str(j) + \" = \" + data[\"choice_list\"][j] + \"; \" for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../submission/answer_sen.txt\", \"r\") as f:\n",
    "    answers = f.readlines()\n",
    "\n",
    "with open(\"chain_of_thought_output.txt\", \"w\") as f:\n",
    "    for r in raw[:29]:\n",
    "        f.write(r)\n",
    "    for i in range(120):\n",
    "        f.write(lines[i*3])\n",
    "        f.write(\"Choices: \" + choices[i] + \"\\n\")\n",
    "        f.write(lines[i*3+1])\n",
    "        answer = answers[i].strip()\n",
    "        f.write(f\"Corresponding Answer: {str(answer)} = {sp_test[i]['choice_list'][int(answers[i])]}\\n\")\n",
    "        f.write(f\"Correct Answer: {label[i]} = {correct[i]}\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 34, 35, 27, 28, 29, 63, 64, 65, 48, 49, 50, 84, 85, 86, 96, 97, 98, 39, 40, 41, 36, 37, 38, 72, 73, 74, 6, 7, 8, 114, 115, 116, 21, 22, 23, 24, 25, 26, 78, 79, 80, 30, 31, 32, 18, 19, 20, 105, 106, 107, 57, 58, 59, 3, 4, 5, 75, 76, 77, 0, 1, 2, 54, 55, 56, 90, 91, 92, 93, 94, 95, 51, 52, 53, 45, 46, 47, 108, 109, 110, 69, 70, 71, 15, 16, 17, 42, 43, 44, 12, 13, 14, 81, 82, 83, 111, 112, 113, 9, 9, 11, 60, 61, 62, 87, 88, 89, 117, 118, 119, 102, 103, 104, 66, 66, 68, 99, 100, 101]\n"
     ]
    }
   ],
   "source": [
    "wrepeatid = []\n",
    "wtest_questions = []\n",
    "worder = []\n",
    "for data in wp_test:\n",
    "    wtest_questions.append(data[\"question\"])\n",
    "for data in wp_all:\n",
    "    if data[\"question\"] in wtest_questions:\n",
    "        wrepeatid.append(data[\"id\"])\n",
    "        m = [i for i in range(len(wtest_questions)) if wtest_questions[i] == data[\"question\"]][0]\n",
    "        worder.append(m)\n",
    "print(worder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Multishot GPT4\n",
    "preds = util.run_eval_cot(client,\"gpt-4\", data, prompts.CHAIN_SP_BASE, prompts.CHAIN_SYSTEM, MULTI_PREFIX, m=0, n=len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67, 89]\n",
      "{1: 0, 2: 2, 3: 2, 4: 1, 5: 3, 6: 0, 7: 0, 8: 1, 9: 3, 10: 1, 11: 1, 12: 3, 13: 2, 14: 0, 15: 1, 16: 1, 17: 0, 18: 2, 19: 2, 20: 0, 21: 0, 22: 0, 23: 1, 24: 0, 25: 1, 26: 0, 27: 0, 28: 2, 29: 0, 30: 2, 31: 3, 32: 2, 33: 2, 34: 2, 35: 3, 36: 2, 37: 3, 38: 0, 39: 0, 40: 0, 41: 1, 42: 3, 43: 0, 44: 0, 45: 2, 46: 2, 47: 2, 48: 2, 49: 0, 50: 0, 51: 0, 52: 0, 53: 3, 54: 1, 55: 2, 56: 0, 57: 1, 58: 1, 59: 1, 60: 1, 61: 3, 62: 1, 63: 1, 64: 1, 65: 1, 66: 2, 68: 1, 69: 2, 70: 0, 71: 3, 72: 0, 73: 3, 74: 2, 75: 1, 76: 2, 77: 0, 78: 1, 79: 2, 80: 0, 81: 3, 82: 2, 83: 3, 84: 3, 85: 3, 86: 1, 87: 2, 88: 3, 90: 2, 91: 1, 92: 0, 93: 3, 94: 0, 95: 2, 96: 3, 97: 1, 98: 2, 99: 0, 100: 2, 101: 1, 102: 3, 103: 0, 104: 2, 105: 2, 106: 2, 107: 3, 108: 1, 109: 2, 110: 1, 111: 1, 112: 1, 113: 1, 114: 3, 115: 2, 116: 1, 117: 2, 118: 1, 119: 1, 120: 2}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open(\"test_logs/gpt-4_eval_2024-01-30_18-17-18.log\", \"r\") as f:\n",
    "    raw = f.readlines()\n",
    "lines = raw[30:]\n",
    "# Copy answers and ids\n",
    "ans = {}\n",
    "for i in range(len(lines)-1):\n",
    "    m = re.search(r\"Question [0-9]+:\", lines[i])\n",
    "    if m is not None:\n",
    "        n = m.group(0)\n",
    "        id = int(re.search(r\"[0-9]+\", n).group(0))\n",
    "        an = re.search(r\"answer is [0-3]\", lines[i+1])\n",
    "        if an is not None:\n",
    "            an = int(an.group(0)[-1])\n",
    "            ans[id] = an\n",
    "notin = []\n",
    "for i in range(1, 120):\n",
    "    if i not in ans:\n",
    "        notin.append(i)\n",
    "print(notin)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../submission/answer_sen.txt\", \"w\") as f:\n",
    "    for i in range(1, 121):\n",
    "        if i in ans:\n",
    "            f.write(str(ans[i]) + \"\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m120\u001b[39m):\n\u001b[1;32m      8\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(lines[i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m----> 9\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChoices: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(lines[i\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     11\u001b[0m     answer \u001b[38;5;241m=\u001b[39m answers[i]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "with open(\"../submission/answer_sen.txt\", \"r\") as f:\n",
    "    answers = f.readlines()\n",
    "\n",
    "with open(\"chain_of_thought_gpt4.txt\", \"w\") as f:\n",
    "    for r in raw[:29]:\n",
    "        f.write(r)\n",
    "    for i in range(120):\n",
    "        f.write(lines[i*3])\n",
    "        f.write(\"Choices: \" + choices[i] + \"\\n\")\n",
    "        f.write(lines[i*3+1])\n",
    "        answer = answers[i].strip()\n",
    "        f.write(f\"Corresponding Answer: {str(answer)} = {sp_test[i]['choice_list'][int(answers[i])]}\\n\")\n",
    "        f.write(f\"Correct Answer: {label[i]} = {correct[i]}\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Zeroshot GPT3.5-TURBO\n",
    "preds = util.run_eval_cot(client, MODEL, sp_test, prompts.ZERO_SP_BASE, prompts.CHAIN_SYSTEM, MULTI_PREFIX, m=0, n=len(sp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 33, 43, 72, 86, 93, 100, 107, 112]\n",
      "{8: [2], 9: [2], 10: [2], 11: [2], 12: [0], 13: [1], 14: [3], 15: [0], 16: [0], 17: [1], 18: [2], 19: [0], 20: [2], 21: [2], 22: [0], 23: [0], 24: [0], 25: [2], 26: [0], 27: [2], 28: [3], 29: [0], 30: [1], 31: [2], 32: [3], 34: [3, 1], 35: [0], 36: [3], 37: [2], 38: [2], 39: [1], 40: [2], 41: [1], 42: [0], 44: [0], 45: [3], 46: [0], 47: [1], 48: [2], 49: [2], 50: [2], 51: [1], 52: [1], 53: [1], 54: [3], 55: [2], 56: [2], 57: [0], 58: [0], 59: [3], 60: [1], 61: [2], 62: [1], 63: [0], 64: [0], 65: [3], 66: [1], 67: [0], 68: [2], 69: [1], 70: [3], 71: [2], 73: [0], 74: [2], 75: [2], 76: [2], 77: [3], 78: [2], 79: [2], 80: [0, 0], 81: [2], 82: [3], 83: [2], 84: [3], 85: [2], 87: [3, 0, 1, 2], 88: [2], 89: [2], 90: [2], 91: [3], 92: [3], 94: [0], 95: [0], 96: [1], 97: [3], 98: [2], 99: [3], 101: [3], 102: [2], 103: [2], 104: [3, 3], 105: [0], 106: [2], 108: [0], 109: [0], 110: [2], 111: [0], 113: [3], 114: [2], 115: [3], 116: [3], 117: [2], 118: [2, 2], 119: [2], 120: [3]}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open(\"test_logs/gpt-3.5-turbo_eval_2023-12-19_20-02-28.log\", \"r\") as f:\n",
    "    raw = f.readlines()\n",
    "lines = raw[30:]\n",
    "# Copy answers and ids\n",
    "ans = {}\n",
    "for i in range(len(lines)-1):\n",
    "    m = re.search(r\"Question [0-9]+:\", lines[i])\n",
    "    if m is not None:\n",
    "        n = m.group(0)\n",
    "        id = int(re.search(r\"[0-9]+\", n).group(0))\n",
    "        an = re.findall(r\"[^0-9][0-3][^0-9]\", lines[i+1])\n",
    "        if len(an) > 0:\n",
    "            an = [int(i[1]) for i in an]\n",
    "            ans[id] = an\n",
    "notin = []\n",
    "for i in range(1, 120):\n",
    "    if i not in ans:\n",
    "        notin.append(i)\n",
    "print(notin)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../submission/answer_sen.txt\", \"w\") as f:\n",
    "    for i in range(1, 121):\n",
    "        if i in ans:\n",
    "            if len(ans[i]) == 1:\n",
    "                f.write(str(ans[i][0]) + \"\\n\")\n",
    "            else:\n",
    "                f.write(\"\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8416666666666667\n"
     ]
    }
   ],
   "source": [
    "# Accuracy testing\n",
    "with open(\"../submission/answer_sen.txt\", \"r\") as f:\n",
    "    answers = f.readlines()\n",
    "with open(\"../data/new_test_data_nolabel/sp_choices.txt\", \"r\") as f:\n",
    "    choices = f.readlines()\n",
    "\n",
    "for i in range(len(answers)):\n",
    "    answers[i] = int(answers[i].strip())\n",
    "    choices[i] = int(choices[i].strip())\n",
    "\n",
    "acc = 0\n",
    "for i in range(len(answers)):\n",
    "    if answers[i] == choices[i]:\n",
    "        acc += 1\n",
    "print(acc/len(answers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
