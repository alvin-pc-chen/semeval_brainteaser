{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import util\n",
    "import random\n",
    "import numpy as np\n",
    "import keys\n",
    "import os\n",
    "import random\n",
    "from openai import OpenAI\n",
    "import constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded WP-train.npy at index 0\n",
      "Loaded SP_eval_data_for_practice.npy at index 1\n",
      "Loaded SP-train.npy at index 2\n",
      "Loaded WP_eval_data_for_practice.npy at index 3\n",
      "Loaded ada-sp_all_2023-12-07.npy at index 0\n",
      "Loaded ada-wp_all_2023-12-07.npy at index 1\n"
     ]
    }
   ],
   "source": [
    "# Load data and Initialize OpenAI\n",
    "os.environ['OPENAI_API_KEY'] = keys.WX\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "dataset, datakey = util.load_data()\n",
    "#TODO split data\n",
    "wp_all = dataset[0]\n",
    "# sp_test = dataset[1]\n",
    "sp_all = dataset[2]\n",
    "# wp_test = dataset[3]\n",
    "wp_base, wp_sr, wp_cr = util.split_data_by_type(wp_all)\n",
    "sp_base, sp_sr, sp_cr = util.split_data_by_type(sp_all)\n",
    "\n",
    "# Load embeddings\n",
    "embedset, embedkey = util.load_data(path=constants.EMBEDDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "embeddings = []\n",
    "ids = []\n",
    "labels = []\n",
    "for i in embedset[0]:\n",
    "    embeddings.append(i[\"embedding\"])\n",
    "    ids.append(i[\"id\"])\n",
    "    labels.append(i[\"label\"])\n",
    "data = list(zip(embeddings, ids, labels))\n",
    "train, dev = util.split_data(data, 0.8)\n",
    "sp_train_embeddings, sp_train_ids, sp_train_labels = zip(*train)\n",
    "sp_dev_embeddings, sp_dev_ids, sp_dev_labels = zip(*dev)\n",
    "sp_train_embeddings = list(sp_train_embeddings)\n",
    "sp_dev_embeddings = list(sp_dev_embeddings)\n",
    "sp_train_ids = list(sp_train_ids)\n",
    "sp_dev_ids = list(sp_dev_ids)\n",
    "sp_train_labels = list(sp_train_labels)\n",
    "sp_dev_labels = list(sp_dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try linear classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(sp_train_embeddings, sp_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4803921568627451\n",
      "F1:  0.3874779507630953\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "predictions = clf.predict(sp_dev_embeddings)\n",
    "f1 = f1_score(sp_dev_labels, predictions, average='macro')\n",
    "accuracy = accuracy_score(sp_dev_labels, predictions)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"F1: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log results\n",
    "answers = [int(i) for i in predictions]\n",
    "util.log_results(answers,\n",
    "                sp_dev_ids,\n",
    "                sp_all,\n",
    "                \"sklearn-linear\",\n",
    "                f1,\n",
    "                accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Naive Bayes? CRF? BiLSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "SYSTEM_PROMPT = {\"role\": \"system\", \n",
    "                 \"content\": \"You are a Question Answering Model, your response must be a number from the choices that are delimited by the symbol \\\";\\\" .\"}\n",
    "MULTI_PREFIX = \"You are a Question Answering Model, your response must be a number from the choices that are delimited by the symbol \\\";\\\". Here are some examples: \\n\"\n",
    "SP_QUESTION = \"Think outside of the box and respond with the number corresponding to the best choice for the following question.\\nQuestion: \"\n",
    "WP_QUESTION = \"For the following word problem, look at the meaning and letters in the words and respond with the number corresponding to the best choice.\\nQuestion: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP F1:  0.4392703201970444\n",
      "SP Accuracy:  0.52\n"
     ]
    }
   ],
   "source": [
    "# Multi Shot\n",
    "wp_train, wp_dev = util.split_data(wp_all, 0.5)\n",
    "f1, accuracy = util.run_test_nofinetune(client, MODEL, wp_dev, WP_QUESTION, SYSTEM_PROMPT, MULTI_PREFIX, training_data=wp_train, m=15, n=50)\n",
    "print(\"SP F1: \", f1)\n",
    "print(\"SP Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
