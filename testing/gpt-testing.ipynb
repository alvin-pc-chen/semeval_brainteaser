{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from openai import OpenAI\n",
    "\n",
    "import os\n",
    "\n",
    "from utils import constants\n",
    "from utils import util\n",
    "from utils import keys\n",
    "from utils import prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded WP-train.npy at index 0\n",
      "Loaded SP_eval_data_for_practice.npy at index 1\n",
      "Loaded SP-train.npy at index 2\n",
      "Loaded WP_eval_data_for_practice.npy at index 3\n"
     ]
    }
   ],
   "source": [
    "# Load data and Initialize OpenAI\n",
    "os.environ['OPENAI_API_KEY'] = keys.WX\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "dataset, datakey = util.load_data()\n",
    "wp_all = dataset[0]\n",
    "sp_test = dataset[1]\n",
    "sp_all = dataset[2]\n",
    "wp_test = dataset[3]\n",
    "# wp_base, wp_sr, wp_cr = util.split_data_by_type(wp_all)\n",
    "# sp_base, sp_sr, sp_cr = util.split_data_by_type(sp_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "MODEL = \"gpt-3.5-turbo\"\n",
    "SYSTEM_PROMPT = {\"role\": \"system\", \n",
    "                 \"content\": \"You are a Question Answering Model, your response must be a number from the choices that are delimited by the symbol \\\";\\\" .\"}\n",
    "MULTI_PREFIX = \"You are a Question Answering Model, your response must be a number from the choices that are delimited by the symbol \\\";\\\". Here are some examples: \\n\"\n",
    "SP_QUESTION = \"Think outside of the box and respond with the number corresponding to the best choice for the following question.\\nQuestion: \"\n",
    "WP_QUESTION = \"For the following word problem, look at the meaning and letters in the words and respond with the number corresponding to the best choice.\\nQuestion: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Multishot GPT3.5-TURBO\n",
    "preds = util.run_eval_cot(client, MODEL, sp_test, prompts.CHAIN_SP_BASE, prompts.CHAIN_SYSTEM, MULTI_PREFIX, m=0, n=len(sp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission to /Users/alvinchen/Documents/GitHub/brainteaser-data/submission/answer_sen.txt\n"
     ]
    }
   ],
   "source": [
    "util.submission_log(preds, \"answer_sen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(\"test_logs/gpt-3.5-turbo_eval_2023-12-18_21-48-05.log\", \"r\") as f:\n",
    "    raw = f.readlines()\n",
    "lines = raw[30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy answers and ids\n",
    "ans = {}\n",
    "for i in range(len(lines)-1):\n",
    "    m = re.search(r\"Question [0-9]+:\", lines[i])\n",
    "    if m is not None:\n",
    "        n = m.group(0)\n",
    "        id = int(re.search(r\"[0-9]+\", n).group(0))\n",
    "        an = re.search(r\"answer is [0-3]\", lines[i+1])\n",
    "        if an is not None:\n",
    "            an = int(an.group(0)[-1])\n",
    "            ans[id] = an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 23, 26, 28, 31, 34, 38, 58, 59, 92, 95, 114]\n"
     ]
    }
   ],
   "source": [
    "notin = []\n",
    "for i in range(1, 120):\n",
    "    if i not in ans:\n",
    "        notin.append(i)\n",
    "print(notin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 47, 98, 39, 5, 23, 3, 102, 51, 28, 50, 77, 1, 24, 88, 94, 71, 85, 11, 111, 93, 86, 119, 66, 104, 108, 61, 60, 116, 73, 59, 110, 52, 72, 36, 17, 92, 21, 69, 62, 113, 107, 90, 81, 112, 35, 16, 57, 19, 40, 43, 70, 46, 79, 13, 83, 64, 18, 49, 27, 58, 115, 31, 33, 2, 109, 6, 9, 8, 97, 84, 41, 89, 10, 101, 91, 53, 106, 55, 117, 82, 99, 37, 56, 87, 105, 44, 100, 67, 22, 20, 15, 12, 103, 75, 4, 7, 14, 29, 34, 32, 42, 65, 95, 25, 96, 114, 30, 54, 118, 63, 76, 0, 26, 74, 78, 45, 80, 68, 48]\n"
     ]
    }
   ],
   "source": [
    "repeatid = []\n",
    "test_questions = []\n",
    "order = []\n",
    "for data in sp_test:\n",
    "    test_questions.append(data[\"question\"])\n",
    "for data in sp_all:\n",
    "    if data[\"question\"] in test_questions:\n",
    "        repeatid.append(data[\"id\"])\n",
    "        m = [i for i in range(len(test_questions)) if test_questions[i] == data[\"question\"]][0]\n",
    "        order.append(m)\n",
    "print(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = []\n",
    "label = []\n",
    "choices = []\n",
    "for i in order:\n",
    "    id = repeatid[i]\n",
    "    for data in sp_all:\n",
    "        if data[\"id\"] == id:\n",
    "            correct.append(data[\"answer\"])\n",
    "            label.append(data[\"label\"])\n",
    "            choices.append(\"\".join(str(j) + \" = \" + data[\"choice_list\"][j] + \"; \" for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../submission/answer_sen.txt\", \"r\") as f:\n",
    "    answers = f.readlines()\n",
    "\n",
    "with open(\"chain_of_thought_output.txt\", \"w\") as f:\n",
    "    for r in raw[:29]:\n",
    "        f.write(r)\n",
    "    for i in range(120):\n",
    "        f.write(lines[i*3])\n",
    "        f.write(\"Choices: \" + choices[i] + \"\\n\")\n",
    "        f.write(lines[i*3+1])\n",
    "        answer = answers[i].strip()\n",
    "        f.write(f\"Corresponding Answer: {str(answer)} = {sp_test[i]['choice_list'][int(answers[i])]}\\n\")\n",
    "        f.write(f\"Correct Answer: {label[i]} = {correct[i]}\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 34, 35, 27, 28, 29, 63, 64, 65, 48, 49, 50, 84, 85, 86, 96, 97, 98, 39, 40, 41, 36, 37, 38, 72, 73, 74, 6, 7, 8, 114, 115, 116, 21, 22, 23, 24, 25, 26, 78, 79, 80, 30, 31, 32, 18, 19, 20, 105, 106, 107, 57, 58, 59, 3, 4, 5, 75, 76, 77, 0, 1, 2, 54, 55, 56, 90, 91, 92, 93, 94, 95, 51, 52, 53, 45, 46, 47, 108, 109, 110, 69, 70, 71, 15, 16, 17, 42, 43, 44, 12, 13, 14, 81, 82, 83, 111, 112, 113, 9, 9, 11, 60, 61, 62, 87, 88, 89, 117, 118, 119, 102, 103, 104, 66, 66, 68, 99, 100, 101]\n"
     ]
    }
   ],
   "source": [
    "wrepeatid = []\n",
    "wtest_questions = []\n",
    "worder = []\n",
    "for data in wp_test:\n",
    "    wtest_questions.append(data[\"question\"])\n",
    "for data in wp_all:\n",
    "    if data[\"question\"] in wtest_questions:\n",
    "        wrepeatid.append(data[\"id\"])\n",
    "        m = [i for i in range(len(wtest_questions)) if wtest_questions[i] == data[\"question\"]][0]\n",
    "        worder.append(m)\n",
    "print(worder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Multishot GPT4\n",
    "preds = util.run_eval_cot(client,\"gpt-4\", sp_test, prompts.CHAIN_SP_BASE, prompts.CHAIN_SYSTEM, MULTI_PREFIX, m=0, n=len(sp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116]\n",
      "{1: 2, 2: 2, 3: 1, 4: 0, 5: 3, 6: 1, 7: 1, 8: 2, 9: 2, 10: 1, 11: 0, 12: 1, 13: 1, 14: 2, 15: 0, 16: 0, 17: 1, 18: 2, 19: 1, 20: 1, 21: 2, 22: 1, 23: 3, 24: 0, 25: 2, 26: 0, 27: 0, 28: 0, 29: 1, 30: 0, 31: 2, 32: 0, 33: 0, 34: 2, 35: 1, 36: 2, 37: 0, 38: 0, 39: 1, 40: 2, 41: 2, 42: 0, 43: 2, 44: 0, 45: 2, 46: 0, 47: 1, 48: 2, 49: 2, 50: 0, 51: 1, 52: 1, 53: 1, 54: 2, 55: 2, 56: 0, 57: 2, 58: 0, 59: 1, 60: 1, 61: 0, 62: 3, 63: 0, 64: 3, 65: 2, 66: 1, 67: 1, 68: 2, 69: 0, 70: 1, 71: 2, 72: 2, 73: 2, 74: 1, 75: 2, 76: 3, 77: 1, 78: 2, 79: 2, 80: 0, 81: 2, 82: 0, 83: 1, 84: 1, 85: 2, 86: 2, 87: 0, 88: 2, 89: 2, 90: 2, 91: 1, 92: 0, 93: 1, 94: 0, 95: 0, 96: 0, 97: 0, 98: 2, 99: 0, 100: 0, 101: 2, 102: 2, 103: 2, 104: 2, 105: 2, 106: 2, 107: 1, 108: 0, 109: 1, 110: 0, 111: 1, 112: 1, 113: 2, 114: 0, 115: 2, 117: 0, 118: 3, 119: 0, 120: 2}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open(\"test_logs/gpt-4_eval_2023-12-19_06-33-46.log\", \"r\") as f:\n",
    "    raw = f.readlines()\n",
    "lines = raw[30:]\n",
    "# Copy answers and ids\n",
    "ans = {}\n",
    "for i in range(len(lines)-1):\n",
    "    m = re.search(r\"Question [0-9]+:\", lines[i])\n",
    "    if m is not None:\n",
    "        n = m.group(0)\n",
    "        id = int(re.search(r\"[0-9]+\", n).group(0))\n",
    "        an = re.search(r\"answer is [0-3]\", lines[i+1])\n",
    "        if an is not None:\n",
    "            an = int(an.group(0)[-1])\n",
    "            ans[id] = an\n",
    "notin = []\n",
    "for i in range(1, 120):\n",
    "    if i not in ans:\n",
    "        notin.append(i)\n",
    "print(notin)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../submission/answer_sen.txt\", \"w\") as f:\n",
    "    for i in range(1, 121):\n",
    "        if i in ans:\n",
    "            f.write(str(ans[i]) + \"\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../submission/answer_sen.txt\", \"r\") as f:\n",
    "    answers = f.readlines()\n",
    "\n",
    "with open(\"chain_of_thought_gpt4.txt\", \"w\") as f:\n",
    "    for r in raw[:29]:\n",
    "        f.write(r)\n",
    "    for i in range(120):\n",
    "        f.write(lines[i*3])\n",
    "        f.write(\"Choices: \" + choices[i] + \"\\n\")\n",
    "        f.write(lines[i*3+1])\n",
    "        answer = answers[i].strip()\n",
    "        f.write(f\"Corresponding Answer: {str(answer)} = {sp_test[i]['choice_list'][int(answers[i])]}\\n\")\n",
    "        f.write(f\"Correct Answer: {label[i]} = {correct[i]}\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Zeroshot GPT3.5-TURBO\n",
    "preds = util.run_eval_cot(client, MODEL, sp_test, prompts.ZERO_SP_BASE, prompts.CHAIN_SYSTEM, MULTI_PREFIX, m=0, n=len(sp_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 33, 43, 72, 86, 93, 100, 107, 112]\n",
      "{8: [2], 9: [2], 10: [2], 11: [2], 12: [0], 13: [1], 14: [3], 15: [0], 16: [0], 17: [1], 18: [2], 19: [0], 20: [2], 21: [2], 22: [0], 23: [0], 24: [0], 25: [2], 26: [0], 27: [2], 28: [3], 29: [0], 30: [1], 31: [2], 32: [3], 34: [3, 1], 35: [0], 36: [3], 37: [2], 38: [2], 39: [1], 40: [2], 41: [1], 42: [0], 44: [0], 45: [3], 46: [0], 47: [1], 48: [2], 49: [2], 50: [2], 51: [1], 52: [1], 53: [1], 54: [3], 55: [2], 56: [2], 57: [0], 58: [0], 59: [3], 60: [1], 61: [2], 62: [1], 63: [0], 64: [0], 65: [3], 66: [1], 67: [0], 68: [2], 69: [1], 70: [3], 71: [2], 73: [0], 74: [2], 75: [2], 76: [2], 77: [3], 78: [2], 79: [2], 80: [0, 0], 81: [2], 82: [3], 83: [2], 84: [3], 85: [2], 87: [3, 0, 1, 2], 88: [2], 89: [2], 90: [2], 91: [3], 92: [3], 94: [0], 95: [0], 96: [1], 97: [3], 98: [2], 99: [3], 101: [3], 102: [2], 103: [2], 104: [3, 3], 105: [0], 106: [2], 108: [0], 109: [0], 110: [2], 111: [0], 113: [3], 114: [2], 115: [3], 116: [3], 117: [2], 118: [2, 2], 119: [2], 120: [3]}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open(\"test_logs/gpt-3.5-turbo_eval_2023-12-19_20-02-28.log\", \"r\") as f:\n",
    "    raw = f.readlines()\n",
    "lines = raw[30:]\n",
    "# Copy answers and ids\n",
    "ans = {}\n",
    "for i in range(len(lines)-1):\n",
    "    m = re.search(r\"Question [0-9]+:\", lines[i])\n",
    "    if m is not None:\n",
    "        n = m.group(0)\n",
    "        id = int(re.search(r\"[0-9]+\", n).group(0))\n",
    "        an = re.findall(r\"[^0-9][0-3][^0-9]\", lines[i+1])\n",
    "        if len(an) > 0:\n",
    "            an = [int(i[1]) for i in an]\n",
    "            ans[id] = an\n",
    "notin = []\n",
    "for i in range(1, 120):\n",
    "    if i not in ans:\n",
    "        notin.append(i)\n",
    "print(notin)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../submission/answer_sen.txt\", \"w\") as f:\n",
    "    for i in range(1, 121):\n",
    "        if i in ans:\n",
    "            if len(ans[i]) == 1:\n",
    "                f.write(str(ans[i][0]) + \"\\n\")\n",
    "            else:\n",
    "                f.write(\"\\n\")\n",
    "        else:\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
